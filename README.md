#Self Driving in Grand Theft Auto V 
==================================

files/directories:
gta.py : Main Train/Test file
memory : Holds all the train data in CSV format.
models : Training models are saved here
Graph  : Tensorbord data are saved here
tes    : Due to GPU limitation seperate gameplay was saved here for testing.


## Self Driving by behaviour cloning.
CNN have revolutionized the computational pattern recognition process. Prior to the widespread adoption of CNNs, most pattern recognition tasks were performed using an initial stage of hand-crafted feature extraction followed by a classifier. The important breakthrough of CNNs is that features are now learned  automatically from training examples. The CNN approach is especially powerful when applied to image recognition tasks because the convolution operation captures the 2D nature of images. The goal is to teach a Convolutional Neural Network (CNN) to drive a bike in a GTAV enviornment. Game screenshots provide video streams and records the keys pressed during game play. Each key will represent a 1 x Action space vectors which will explained later. This turns out to be a regression task, which is very different from usual applications of CNNs for classification purposes. I collected the training data by driving the bike on the outer city loop (freeway). The performance of the CNN can then be checked by letting the bike ride autonomously on the same freeway. Below is small demo ( https://vimeo.com/221087186) .

![Alt text](images/gta.gif?raw=true "GTA Self drive")

## Dataset

The dataset was generated by taking screenshots of the gameplay and the key pressed.
W(forward), S(Stop/Backward), A(Left), D(Right) keys were recorded. key T is for quit.
for each key there is a unique action vector.
No Action: [[0, 0, 0, 0]]
W:         [[1, 0, 0, 0]]
S:         [[0, 0, 0, 1]]
A:         [[0, 1, 0, 0]]
D:         [[0, 0, 1, 0]]

The images were captures in RGB space and in the preprocessing stage images were resized 320x240x3 and converted to YUV space.
where 3 in number of chanels 320x240 is width x height.

Run gta.py with mode = 1 which says to collect data. For a descent action prediction I recommend collecting atleast 8 hours of game play. The training route I chose was the outer city loop since traffic is smooth and I feel this route has less city noise for training data.

While you are playing the game run gta.py on a separate console. The script records yours keys and stores both image and action vectors in csv file in memory. Once you want to quit collecting data just press "T".

## Training
To train the network run gta.py with mode = 2. The load data method will load all memories and sort them randomly. Then it will start loading images and action pair from the dataset. A replay memory of 50000 is set so we can limit of training data for each tarining.
The loaded data will then be shuffeled and split into training and testing.

Here is how the network is designed. The pixel values which vary from 0 to 255 is shrunk into range -1, -1 then fed into 5 convolutaion layer follwed by 3 fully connected dense layers. dropout of 0.5 is also considered between conv and dense layers. Since we are feeding data in mini batches its a stocastic gradient descent. The model used mean sqaured error with adam optimizer for convergence. I havent expriemented with hyperparameters much but for optimizer I think stocastic gradient decent with momentum and nesterov should perform better.

```python
model = Sequential()
model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE))
model.add(Conv2D(24, 5, 5, activation='elu', subsample=(2, 2)))
model.add(Conv2D(36, 5, 5, activation='elu', subsample=(2, 2)))
model.add(Conv2D(48, 5, 5, activation='elu', subsample=(2, 2)))
model.add(Conv2D(64, 3, 3, activation='elu'))
model.add(Conv2D(64, 3, 3, activation='elu'))
model.add(Dropout(args.keep_prob))
model.add(Flatten())
model.add(Dense(100, activation='elu'))
model.add(Dense(50, activation='elu'))
model.add(Dense(10, activation='elu'))
model.add(Dense(ACTIONS))

model.compile(loss='mean_squared_error', optimizer=Adam(lr=args.learning_rate))
``` 
Once the metwork is trained models are stored in models directory. Head`s up, Training takes a lot of time and I recommend to keep it running overnight.

## Testing

Once you see a decent validation loss its time to give it for a spin. Start GTA game and run gta.py with mode = 3. The script loads the network and models. It takes screenshots, preprocess them and feeds to network which outputs an action vector i.e [[0.93747151 0.01762279 0.00507341 0.00577346]]. Pick the index of highest value and perform that action. ie [[1 0 0 0]] W(forward).

Note: To run the GTAV itself most of the GPU is consumed, hence to make run time action prediction is not possibled with low end GPU.
I used GForce GTX 1080 and my GPU throws errors when I do this.
If you have two GPU you can run GTA on one and tensorflow on other (use test() function ). If you have one GPU, then you can seperatly record the game and then feed that gameplay to network and compare the actions you took and actions what the network took (use testgame() function). 

## Additional References
1. https://medium.com/@ksakmann/behavioral-cloning-make-a-car-drive-like-yourself-dc6021152713
2. https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/